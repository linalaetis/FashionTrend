{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_cell"
   },
   "source": [
    "# Guide pour l'Appel √† une API Hugging Face pour la Segmentation d'Images\n",
    "\n",
    "Bienvenue ! Ce notebook a pour but de vous guider pas √† pas dans l'utilisation de l'API d'inf√©rence de Hugging Face pour effectuer de la segmentation d'images. La segmentation d'images consiste √† attribuer une √©tiquette (comme \"cheveux\", \"v√™tement\", \"arri√®re-plan\") √† chaque pixel d'une image.\n",
    "\n",
    "Nous allons :\n",
    "1. Comprendre ce qu'est une API et comment s'y connecter.\n",
    "2. Envoyer une image √† un mod√®le de segmentation h√©berg√© sur Hugging Face.\n",
    "3. R√©cup√©rer et interpr√©ter les r√©sultats.\n",
    "4. Visualiser les masques de segmentation.\n",
    "5. √âtendre cela pour traiter plusieurs images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_intro_cell"
   },
   "source": [
    "## 1. Configuration Initiale et Importations\n",
    "\n",
    "Commen√ßons par importer les biblioth√®ques Python n√©cessaires. Nous aurons besoin de :\n",
    "- `os` pour interagir avec le syst√®me de fichiers (lister les images).\n",
    "- `requests` pour effectuer des requ√™tes HTTP vers l'API.\n",
    "- `PIL (Pillow)` pour manipuler les images.\n",
    "- `matplotlib.pyplot` pour afficher les images et les masques.\n",
    "- `numpy` pour la manipulation des tableaux (les images sont des tableaux de pixels).\n",
    "- `tqdm.notebook` pour afficher une barre de progression (utile pour plusieurs images).\n",
    "- `base64` et `io` pour d√©coder les masques renvoy√©s par l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_code_cell"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_vars_intro_cell"
   },
   "source": [
    "### Variables de Configuration\n",
    "\n",
    "Nous devons d√©finir quelques variables :\n",
    "- `image_dir`: Le chemin vers le dossier contenant vos images. **Assurez-vous de modifier ce chemin si n√©cessaire.**\n",
    "- `max_images`: Le nombre maximum d'images √† traiter (pour ne pas surcharger l'API ou attendre trop longtemps).\n",
    "- `api_token`: Votre jeton d'API Hugging Face. **IMPORTANT : Gardez ce jeton secret !**\n",
    "\n",
    "**Comment obtenir un token API Hugging Face ?**\n",
    "1. Cr√©ez un compte sur [huggingface.co](https://huggingface.co/).\n",
    "2. Allez dans votre profil -> Settings -> Access Tokens.\n",
    "3. Cr√©ez un nouveau token (par exemple, avec le r√¥le \"read\").\n",
    "4. Copiez ce token ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_vars_code_cell"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# initialisation du dossier o√π sont stock√©es mes images\n",
    "# image_dir = r\"D:\\ia\\appli\\fashiontrend\\data\\external\\IMG\" # Exemple : si vous √™tes sur Colab et avez upload√© un dossier\n",
    "\n",
    "image_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()), r\"data\\external\\IMG\"\n",
    ")  # dans mon cas, image_dir = r\"D:\\ia\\appli\\fashiontrend\\data\\external\\IMG\"\n",
    "max_images = 3  # Commen√ßons avec peu d'images, os.getcwd() c'st le repertoire courant\n",
    "\n",
    "# Charge .env et r√©cup√©re mon token de HUGGING \n",
    "load_dotenv()\n",
    "api_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Cr√©ons le dossier d'images s'il n'existe pas (pour l'exemple)\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "    print(f\"Dossier '{image_dir}' cr√©√©. Veuillez y ajouter des images .jpg ou .png.\")\n",
    "else:\n",
    "    print(f\"Dossier '{image_dir}' existant.\")\n",
    "\n",
    "if api_token == \"VOTRE_TOKEN_HUGGING_FACE_ICI\":\n",
    "    print(\n",
    "        \"\\nATTENTION : Vous devez remplacer 'VOTRE_TOKEN_HUGGING_FACE_ICI' par votre token API personnel.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Requis 2) V√©rifier la connexion au compte Hugging Face\n",
    "\n",
    "- On v√©rifie que le token donne bien acc√®s √† ton compte (nom / organisations).\n",
    "- Si erreur : re-v√©rifie la ligne `HUGGINGFACEHUB_API_TOKEN` dans `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "try:\n",
    "    info = whoami(token=api_token)\n",
    "    print(\"üéâ Connexion OK √† Hugging Face\")\n",
    "    print(\"üë§ Utilisateur :\", info.get(\"name\"))\n",
    "    print(\"üè¢ Organisations :\", info.get(\"orgs\"))\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"‚ùå Erreur d'authentification : {e}\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api_understanding_cell"
   },
   "source": [
    "## 2. Comprendre l'API d'Inf√©rence Hugging Face\n",
    "\n",
    "L'API d'inf√©rence permet d'utiliser des mod√®les h√©berg√©s sur Hugging Face sans avoir √† les t√©l√©charger ou √† g√©rer l'infrastructure.\n",
    "\n",
    "- **Mod√®le utilis√©** : Nous allons utiliser le mod√®le `sayeed99/segformer_b3_clothes`, sp√©cialis√© dans la segmentation de v√™tements et de parties du corps.\n",
    "- **URL de l'API** : L'URL pour un mod√®le est g√©n√©ralement `https://api-inference.huggingface.co/models/NOM_DU_MODELE`.\n",
    "- **Headers (En-t√™tes)** : Pour s'authentifier et sp√©cifier le type de contenu, nous envoyons des en-t√™tes avec notre requ√™te.\n",
    "    - `Authorization`: Contient notre token API (pr√©c√©d√© de `Bearer `).\n",
    "    - `Content-Type`: Indique que nous envoyons une image au format JPEG (ou PNG selon le cas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_setup_code_cell"
   },
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/sayeed99/segformer_b3_clothes\"  # Remplacez ... par le bon endpoint.\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\"\n",
    "}  # Le \"Content-Type\" sera ajout√© dynamiquement lors de l'envoi de l'image\n",
    "\n",
    "## V√©rifions que le token est valide en envoyant une requ√™te GET simple\n",
    "reponse = requests.get(API_URL, headers=headers)\n",
    "if reponse.status_code == 200:\n",
    "    print(\"Le token est valide !\")\n",
    "# print(\"Infos du mod√®le :\", response.json())\n",
    "elif reponse.status_code == 401:\n",
    "    print(\"Le token est invalide ou expir√©.\")\n",
    "else:\n",
    "    print(f\"Erreur inattendue : code {reponse.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les chemins des images √† traiter\n",
    "## On parcourt le dossier 'image_dir' et on garde uniquement les fichiers .jpg, .jpeg ou .png\n",
    "valid_extens = [\".jpg\", \".jpeg\", \".png\"]  # extensions accept√©es\n",
    "image_paths = [\n",
    "    str(p) for p in Path(image_dir).iterdir() if p.is_file() and p.suffix.lower() in valid_extens\n",
    "]  # image_paths = image_paths[:max_images], si trop d‚Äôimages, on limite √† 'max_images'\n",
    "if not image_paths:\n",
    "    print(f\"Aucune image trouv√©e dans '{image_dir}'. Veuillez y ajouter des images.\")\n",
    "else:\n",
    "    print(f\"{len(image_paths)} image(s) √† traiter : {image_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les chemins des masques √† traiter\n",
    "mask_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()), r\"data\\external\\Mask\"\n",
    ")  # dans mon cas, mask_dir = r\"D:\\ia\\appli\\fashiontrend\\data\\external\\Mask\"\n",
    "## On parcourt le dossier 'mask_dir' et on garde uniquement les fichiers .jpg, .jpeg ou .png\n",
    "mask_paths = [\n",
    "    str(p) for p in Path(mask_dir).iterdir() if p.is_file() and p.suffix.lower() in valid_extens\n",
    "]\n",
    "\n",
    "## Assurons nous d'avoir des masques dans le dossier 'mask_dir'!\n",
    "if not mask_paths:\n",
    "    print(f\"Aucune image trouv√©e dans '{mask_dir}'. Veuillez y ajouter des images en masque.\")\n",
    "else:\n",
    "    print(f\"{len(mask_paths)} masque(s) √† traiter : {mask_paths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions_intro_cell"
   },
   "source": [
    "## 3. Fonctions Utilitaires pour le Traitement des Masques\n",
    "\n",
    "Le mod√®le que nous utilisons (`sayeed99/segformer_b3_clothes`) renvoie des masques pour diff√©rentes classes (cheveux, chapeau, etc.). Ces masques sont encod√©s en base64. Les fonctions ci-dessous sont fournies pour vous aider √† :\n",
    "1.  `CLASS_MAPPING`: Un dictionnaire qui associe les noms de classes (ex: \"Hat\") √† des identifiants num√©riques.\n",
    "2.  `get_image_dimensions`: R√©cup√©rer les dimensions d'une image.\n",
    "3.  `decode_base64_mask`: D√©coder un masque de base64 en une image (tableau NumPy) et le redimensionner.\n",
    "4.  `create_masks`: Combiner les masques de toutes les classes d√©tect√©es en un seul masque de segmentation final, o√π chaque pixel a la valeur de l'ID de sa classe.\n",
    "\n",
    "**Cette partie est donn√©e car elle est sp√©cifique au format de sortie de ce mod√®le et un peu complexe pour une premi√®re approche.** Lisez-la pour comprendre son r√¥le, mais ne vous attardez pas sur les d√©tails d'impl√©mentation pour l'instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_functions_code_cell"
   },
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "    \"Background\": 0,\n",
    "    \"Hat\": 1,\n",
    "    \"Hair\": 2,\n",
    "    \"Sunglasses\": 3,\n",
    "    \"Upper-clothes\": 4,\n",
    "    \"Skirt\": 5,\n",
    "    \"Pants\": 6,\n",
    "    \"Dress\": 7,\n",
    "    \"Belt\": 8,\n",
    "    \"Left-shoe\": 9,\n",
    "    \"Right-shoe\": 10,\n",
    "    \"Face\": 11,\n",
    "    \"Left-leg\": 12,\n",
    "    \"Right-leg\": 13,\n",
    "    \"Left-arm\": 14,\n",
    "    \"Right-arm\": 15,\n",
    "    \"Bag\": 16,\n",
    "    \"Scarf\": 17,\n",
    "}\n",
    "\n",
    "\n",
    "def get_image_dimensions(img_path):\n",
    "    \"\"\"\n",
    "    Get the dimensions of an image.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (width, height) of the image.\n",
    "    \"\"\"\n",
    "    original_image = Image.open(img_path)\n",
    "    return original_image.size\n",
    "\n",
    "\n",
    "def decode_base64_mask(base64_string, width, height):\n",
    "    \"\"\"\n",
    "    Decode a base64-encoded mask into a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        base64_string (str): Base64-encoded mask.\n",
    "        width (int): Target width.\n",
    "        height (int): Target height.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Single-channel mask array.\n",
    "    \"\"\"\n",
    "    mask_data = base64.b64decode(base64_string)\n",
    "    mask_image = Image.open(io.BytesIO(mask_data))\n",
    "    mask_array = np.array(mask_image)\n",
    "    if len(mask_array.shape) == 3:\n",
    "        mask_array = mask_array[:, :, 0]  # Take first channel if RGB\n",
    "    mask_image = Image.fromarray(mask_array).resize((width, height), Image.NEAREST)\n",
    "    return np.array(mask_image)\n",
    "\n",
    "\n",
    "def create_masks(results, width, height):\n",
    "    \"\"\"\n",
    "    Combine multiple class masks into a single segmentation mask.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of dictionaries with 'label' and 'mask' keys.\n",
    "        width (int): Target width.\n",
    "        height (int): Target height.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Combined segmentation mask with class indices.\n",
    "    \"\"\"\n",
    "    combined_mask = np.zeros((height, width), dtype=np.uint8)  # Initialize with Background (0)\n",
    "\n",
    "    # traitez d'abord les masques autres que ceux d'arri√®re-plan\n",
    "    for result in results:\n",
    "        label = result[\"label\"]\n",
    "        class_id = CLASS_MAPPING.get(label, 0)\n",
    "        if class_id == 0:  # Skip Background\n",
    "            continue\n",
    "        mask_array = decode_base64_mask(result[\"mask\"], width, height)\n",
    "        combined_mask[mask_array > 0] = class_id\n",
    "\n",
    "    # Ex√©cutez ce processus en arri√®re-plan en dernier afin d'√©viter de remplacer inutilement d'autres classes.\n",
    "    # (Bien que le mod√®le fournisse g√©n√©ralement des masques non superpos√©s pour les diff√©rentes classes, √† l'exception du fond)\n",
    "    for result in results:\n",
    "        if result[\"label\"] == \"Background\":\n",
    "            mask_array = decode_base64_mask(result[\"mask\"], width, height)\n",
    "            # Appliquer le fond uniquement si aucune autre classe n'a √©t√© assign√©e\n",
    "            # Cette logique pourrait n√©cessiter des ajustements en fonction de la d√©finition du terme ¬´¬†fond¬†¬ª par le mod√®le.\n",
    "            # Pour ce mod√®le, il semble plus s√ªr de laisser d'abord les autres classes √©craser cette valeur.\n",
    "            # Une application simple comme celle-ci devrait suffire¬†: si le masque de fond indique que le pixel est un √©l√©ment de fond, attribuer la valeur 0.\n",
    "            # Cependant, une approche plus robuste consisterait √† attribuer la valeur de fond uniquement si la valeur de combined_mask est encore 0 (valeur initiale).\n",
    "            combined_mask[mask_array > 0] = 0  # Class ID pour le fond est 0\n",
    "\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "single_image_intro_cell"
   },
   "source": [
    "## 4. Segmentation d'une Seule Image\n",
    "\n",
    "Avant de traiter toutes les images, concentrons-nous sur une seule pour bien comprendre le processus.\n",
    "\n",
    "√âtapes :\n",
    "1.  Choisir une image.\n",
    "2.  Ouvrir l'image en mode binaire (`\"rb\"`) et lire son contenu (`data`).\n",
    "3.  D√©terminer le `Content-Type` (par exemple, `\"image/jpeg\"` ou `\"image/png\"`).\n",
    "4.  Envoyer la requ√™te POST √† l'API avec `requests.post()` en passant l'URL, les headers et les donn√©es.\n",
    "5.  V√©rifier le code de statut de la r√©ponse. Une erreur sera lev√©e si le code n'est pas 2xx (succ√®s) gr√¢ce √† `response.raise_for_status()`.\n",
    "6.  Convertir la r√©ponse JSON en un dictionnaire Python avec `response.json()`.\n",
    "7.  Utiliser nos fonctions `get_image_dimensions` et `create_masks` pour obtenir le masque final.\n",
    "8.  Afficher l'image originale et le masque segment√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single_image_code_cell"
   },
   "outputs": [],
   "source": [
    "if image_paths:\n",
    "    single_image_path = image_paths[0]  # Prenons la premi√®re image de notre liste\n",
    "    print(f\"Traitement de l'image : {single_image_path}\")\n",
    "\n",
    "    try:\n",
    "        # Lire l'image en binaire\n",
    "        # Et mettez le contenu de l'image dans la variable image_data\n",
    "        with open(single_image_path, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "        # Maintenant, utilis√© l'API huggingface\n",
    "        # ainsi que les fonctions donn√©es plus haut pour s√©gmenter vos images.\n",
    "\n",
    "        # 1) Deviner dynamiquement le Content-Type (image/jpeg, image/png, ...)\n",
    "        with Image.open(single_image_path) as img:\n",
    "            content_type = img.get_format_mimetype()\n",
    "\n",
    "        # 2) Construire des headers sp√©cifiques √† cet envoi (on ajoute le Content-Type ici)\n",
    "        headers_with_type = {\"Authorization\": f\"Bearer {api_token}\", \"Content-Type\": content_type}\n",
    "\n",
    "        # 3) Appel √† l'API Inference en POST avec l'image brute\n",
    "        reponse = requests.post(API_URL, headers=headers_with_type, data=image_data, timeout=60)\n",
    "\n",
    "        # 4) V√©rifier le code de statut de la r√©ponse. Une erreur sera lev√©e si le code n'est pas 2xx (succ√®s) gr√¢ce √† response.raise_for_status().\n",
    "        reponse.raise_for_status()\n",
    "\n",
    "        # 5) R√©cup√©rer le JSON de sortie (liste de dicts: {'label': str, 'mask': base64})\n",
    "        results = reponse.json()\n",
    "\n",
    "        # 6) Utiliser nos fonctions get_image_dimensions et create_masks pour obtenir le masque final.\n",
    "        width, height = get_image_dimensions(single_image_path)\n",
    "        segmentation_mask = create_masks(results, width, height)\n",
    "\n",
    "        # 7) obtenir l'image d'origine.\n",
    "        original_image = Image.open(single_image_path)\n",
    "\n",
    "        # 8) (Option p√©dagogique) Visualiser rapidement l'image et le masque\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        ax1.imshow(original_image)\n",
    "        ax1.set_title(\"Image d'origine\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        ax2 = plt.subplot(1, 2, 2)\n",
    "        im = ax2.imshow(segmentation_mask, cmap=\"tab10\", interpolation=\"nearest\")\n",
    "        ax2.set_title(\"Masque Segment√© (indices de classes)\")\n",
    "        ax2.axis(\"off\")\n",
    "        plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "\n",
    "       # plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue : {e}\")\n",
    "else:\n",
    "    print(\"Aucune image √† traiter. V√©rifiez la configuration de 'image_dir' et 'max_images'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_intro_cell"
   },
   "source": [
    "## 5. Segmentation de Plusieurs Images (Batch)\n",
    "\n",
    "Maintenant que nous savons comment traiter une image, nous pouvons cr√©er une fonction pour en traiter plusieurs.\n",
    "Cette fonction va boucler sur la liste `image_paths` et appliquer la logique de segmentation √† chaque image.\n",
    "Nous utiliserons `tqdm` pour avoir une barre de progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_images_batch(list_of_image_paths, pause=0.5):\n",
    "    \"\"\"\n",
    "    Segmente une liste d'images en utilisant l'API Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        list_of_image_paths (list): Liste des chemins vers les images.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste des masques de segmentation (tableaux NumPy).\n",
    "              Contient None si une image n'a pas pu √™tre trait√©e.\n",
    "    \"\"\"\n",
    "\n",
    "    import time\n",
    "\n",
    "    batch_segmentations = []\n",
    "    temps_reponse = []\n",
    "\n",
    "    for img_path in tqdm(list_of_image_paths, desc=\"Segmentation (HF)\", unit=\"img\"):\n",
    "        try:\n",
    "            # 1) Lire l'image en binaire et son contenu\n",
    "            with open(img_path, \"rb\") as file:\n",
    "                image_data = file.read()\n",
    "\n",
    "            # 2) Deviner dynamiquement le Content-Type (image/jpeg, image/png, ...)\n",
    "            with Image.open(img_path) as img:\n",
    "                content_type = img.get_format_mimetype()\n",
    "                # Construire des headers sp√©cifiques √† cet envoi (on ajoute le Content-Type ici)\n",
    "            headers_with_type = {\n",
    "                \"Authorization\": f\"Bearer {api_token}\",\n",
    "                \"Content-Type\": content_type,\n",
    "            }\n",
    "\n",
    "            temps_debut = time.time()\n",
    "            # 3) Appel API Inference et verifie le statut de la requete\n",
    "            reponse = requests.post(API_URL, headers=headers_with_type, data=image_data, timeout=60)\n",
    "            reponse.raise_for_status()\n",
    "\n",
    "            temps_fin = time.time()\n",
    "            duree = temps_fin - temps_debut\n",
    "            temps_reponse.append(duree)\n",
    "\n",
    "            # 4) R√©sultats JSON attendus: convertis en liste de dicts [{'label':..., 'mask':...}, ...]\n",
    "            results = reponse.json()\n",
    "\n",
    "            # 5) Dimensions de l'image d'origine\n",
    "            width, height = get_image_dimensions(img_path)\n",
    "\n",
    "            # 6) Masque combin√© (indices de classes)\n",
    "            segmentation_mask = create_masks(results, width, height)\n",
    "            batch_segmentations.append(segmentation_mask)\n",
    "\n",
    "            time.sleep(pause)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Erreur dans le traitement de l'image {img_path} : {e}\")\n",
    "            batch_segmentations.append(None)\n",
    "\n",
    "    return batch_segmentations\n",
    "\n",
    "    # Appeler la fonction pour segmenter les images list√©es dans image_paths\n",
    "\n",
    "\n",
    "if image_paths:\n",
    "    print(f\"\\nTraitement de {len(image_paths)} image(s) en batch...\")\n",
    "    batch_seg_results = segment_images_batch(image_paths)\n",
    "    print(\"Traitement en batch termin√©.\")\n",
    "else:\n",
    "    batch_seg_results = []\n",
    "    print(\"Aucune image √† traiter en batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "display_batch_intro_cell"
   },
   "source": [
    "## 6. Affichage des R√©sultats en Batch\n",
    "\n",
    "Nous allons maintenant cr√©er une fonction pour afficher les images originales et leurs segmentations correspondantes c√¥te √† c√¥te, dans une grille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colormap personnalis√© pour 18 classes (valeurs 0 √† 17) en RGB\n",
    "custom_colormap = {\n",
    "    1: (255, 255, 0),  # Jaune - Hat\n",
    "    2: (255, 165, 0),  # Orange - Hair\n",
    "    3: (255, 0, 255),  # Magenta - Sunglasses\n",
    "    4: (255, 0, 0),  # Rouge - Upper-clothes\n",
    "    5: (0, 255, 255),  # Cyan - Skirt\n",
    "    6: (0, 255, 0),  # Vert - Pants\n",
    "    7: (0, 0, 255),  # Bleu - Dress\n",
    "    8: (128, 0, 128),  # Violet - Belt\n",
    "    9: (255, 255, 0),  # Jaune - Left-shoe\n",
    "    10: (255, 140, 0),  # Orange fonc√© - Right-shoe\n",
    "    11: (140, 180, 200),  # Beige - Face\n",
    "    12: (140, 180, 200),  # Beige - Left-leg\n",
    "    13: (140, 180, 200),  # Beige - Right-leg\n",
    "    14: (140, 180, 200),  # Beige - Left-arm\n",
    "    15: (140, 180, 200),  # Beige - Right-arm\n",
    "    16: (0, 128, 255),  # Bleu clair - Bag\n",
    "    17: (255, 20, 147),  # Rose - Scarf\n",
    "}\n",
    "\n",
    "# L√©gendes associ√©es aux labels\n",
    "legend_labels = {\n",
    "    \"0\": \"Background\",\n",
    "    \"1\": \"Hat\",\n",
    "    \"2\": \"Hair\",\n",
    "    \"3\": \"Sunglasses\",\n",
    "    \"4\": \"Upper-clothes\",\n",
    "    \"5\": \"Skirt\",\n",
    "    \"6\": \"Pants\",\n",
    "    \"7\": \"Dress\",\n",
    "    \"8\": \"Belt\",\n",
    "    \"9\": \"Left-shoe\",\n",
    "    \"10\": \"Right-shoe\",\n",
    "    \"11\": \"Face\",\n",
    "    \"12\": \"Left-leg\",\n",
    "    \"13\": \"Right-leg\",\n",
    "    \"14\": \"Left-arm\",\n",
    "    \"15\": \"Right-arm\",\n",
    "    \"16\": \"Bag\",\n",
    "    \"17\": \"Scarf\",\n",
    "}\n",
    "\n",
    "\n",
    "def colorize_mask(mask, colormap):\n",
    "    \"\"\"\n",
    "    Applique le colormap personnalis√© au masque.\n",
    "    Pour chaque pixel, s'il correspond √† un label d√©fini dans colormap,\n",
    "    la couleur correspondante est assign√©e.\n",
    "    \"\"\"\n",
    "    colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for label, color in colormap.items():\n",
    "        colored_mask[mask == label] = color\n",
    "    return colored_mask\n",
    "\n",
    "\n",
    "def display_segmented_images_batch_color(\n",
    "    original_image_paths, segmentation_masks, original_mask_paths, colors, labels\n",
    "):\n",
    "    \"\"\"\n",
    "    Affiche les images originales et les masques originales et segment√©s.\n",
    "\n",
    "    Args:\n",
    "        original_image_paths (list): Liste des chemins des images originales.\n",
    "        original_mask_paths (list): Liste des chemins des masques originales.\n",
    "        segmentation_masks (list): Liste des masques segment√©s (NumPy arrays).\n",
    "        colors - colormap\n",
    "        labels - legend labels\n",
    "    \"\"\"\n",
    "\n",
    "    nb_cols = 3\n",
    "    nb_images = len(original_image_paths)\n",
    "    if nb_images == 0:\n",
    "        print(\"Aucune image √† afficher.\")\n",
    "        return\n",
    "\n",
    "    # on cr√©√© une figure : 4 pouces de hauteur par image, 12 pouces de largeur au total, 3 colonnes (Original | Masque Segment√©  | Masque Original ) par image\n",
    "    fig, axes = plt.subplots(\n",
    "        nb_images, nb_cols, figsize=(12, 4 * nb_images), squeeze=False, constrained_layout=True\n",
    "    )\n",
    "\n",
    "    for i, (img_path, mask_seg, mask_orig) in enumerate(\n",
    "        zip(original_image_paths, segmentation_masks, original_mask_paths, strict=False)\n",
    "    ):\n",
    "        if mask_seg is None:\n",
    "            continue  # Ignore les images avec masque non g√©n√©r√©\n",
    "\n",
    "        # --- Colonne 1 : Image originale\n",
    "        img = Image.open(img_path)\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(f\"Original\\n{Path(img_path).name}\", fontsize=10)\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Masque segment√©\n",
    "        colored_mask = colorize_mask(mask_seg, colors)\n",
    "        ax_seg = axes[i, 1]\n",
    "        ax_seg.imshow(colored_mask)\n",
    "        ax_seg.set_title(\"Segmentation\", fontsize=10)\n",
    "        ax_seg.axis(\"off\")\n",
    "        ## Trouver quelles classes sont pr√©sentes dans ce masque\n",
    "        classes_in_mask = np.unique(mask_seg)\n",
    "        ## Pr√©parer les patches pour ces classes\n",
    "        patches = []\n",
    "        for c in classes_in_mask:\n",
    "            if c == 0:  # ignore background\n",
    "                continue\n",
    "            color = colors.get(c, (0, 0, 0))\n",
    "            color_norm = (color[0] / 255, color[1] / 255, color[2] / 255)\n",
    "            patch = mpatches.Patch(color=color_norm, label=labels.get(str(c), f\"Class {c}\"))\n",
    "            patches.append(patch)\n",
    "        # Afficher la l√©gende localement dans la 2e colonne (masques segment√©s)\n",
    "        ax_seg.legend(\n",
    "            handles=patches, loc=\"upper left\", labelcolor=\"white\", fontsize=\"x-small\", frameon=False\n",
    "        )\n",
    "\n",
    "        # Masque d'origine\n",
    "        msk = Image.open(mask_orig)\n",
    "        msk = np.array(msk, dtype=np.uint8)\n",
    "        colored_mask_orig = colorize_mask(msk, colors)\n",
    "        ax_seg = axes[i, 2]\n",
    "        ax_seg.imshow(colored_mask_orig)\n",
    "        ax_seg.set_title(\"Masque d Origine\")\n",
    "        ax_seg.axis(\"off\")\n",
    "        ## Trouver quelles classes sont pr√©sentes dans ce masque\n",
    "        classes_in_mask_orig = np.unique(msk)\n",
    "        ## Pr√©parer les patches pour ces classes\n",
    "        patches = []\n",
    "        for c in classes_in_mask_orig:\n",
    "            if c == 0:  # ignore background\n",
    "                continue\n",
    "            color = colors.get(c, (0, 0, 0))\n",
    "            color_norm = (color[0] / 255, color[1] / 255, color[2] / 255)\n",
    "            patch = mpatches.Patch(color=color_norm, label=labels.get(str(c), f\"Class {c}\"))\n",
    "            patches.append(patch)\n",
    "        ## Afficher la l√©gende localement dans la 3e colonne (masques d'origine)\n",
    "        ax_seg.legend(\n",
    "            handles=patches, loc=\"upper left\", labelcolor=\"white\", fontsize=\"x-small\", frameon=False\n",
    "        )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Afficher les r√©sultats du batch\n",
    "if batch_seg_results:\n",
    "    display_segmented_images_batch_color(\n",
    "        image_paths, batch_seg_results, mask_paths, custom_colormap, legend_labels\n",
    "    )\n",
    "else:\n",
    "    print(\"Aucun r√©sultat de segmentation √† afficher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcul des m√©triques\n",
    "\n",
    "Faisons un chiffrage approximatif du co√ªt d‚Äôutilisation de cette solution avec l‚ÄôAPI de Hugging Face. Une √©valuation du co√ªt pour 500 000 images sur 30 jours sera faite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\"\"\" Calculate metrics for model evaluation\n",
    "\n",
    " y_true and y_pred: flattened arrays of pixel labels\n",
    " num_classes = 18\n",
    "\"\"\"\n",
    "\n",
    "def segmentation_metrics(mask_true, mask_pred, num_classes=18) -> Dict[str, float]:\n",
    "\n",
    "    mask_true = np.array(mask_true).flatten()\n",
    "    mask_pred = np.array(mask_pred).flatten()\n",
    "    \n",
    "    cm = confusion_matrix(mask_true, mask_pred, labels=np.arange(num_classes))\n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    pixel_acc = TP.sum() / cm.sum()\n",
    "    mean_acc = np.mean(TP / (TP + FN + 1e-10))\n",
    "    iou = TP / (TP + FP + FN + 1e-10)\n",
    "    mean_iou = np.mean(iou)\n",
    "    dice = 2*TP / (2*TP + FP + FN + 1e-10)\n",
    "    precision = TP / (TP + FP + 1e-10)\n",
    "    recall = TP / (TP + FN + 1e-10)\n",
    "\n",
    "    return {\n",
    "        \"Pixel Acc\": pixel_acc,\n",
    "        \"Mean Acc\": mean_acc,\n",
    "        \"mIoU\": mean_iou,\n",
    "        \"Dice\": dice.mean(),\n",
    "        \"Per-class IoU\": iou,\n",
    "        \"Per-class Precision\": precision,\n",
    "        \"Per-class Recall\": recall\n",
    "    }\n",
    "\n",
    "iou_per_image = []\n",
    "\n",
    "if batch_seg_results:\n",
    "    \n",
    "    for i, (mask_orig, mask_seg) in enumerate(zip(mask_paths, batch_seg_results)):\n",
    "        if mask_seg is None:\n",
    "            continue  # Ignore les images avec masque non g√©n√©r√©\n",
    "            \n",
    "        # masque orig\n",
    "        msk = Image.open(mask_orig)  \n",
    "        msk_array = np.array(msk)\n",
    "        \n",
    "        metrics = segmentation_metrics(msk_array, mask_seg, num_classes=18)\n",
    "        #print(f\"Image {i}: {metrics}\")\n",
    "        iou_dict = {f\"class_{c}\": iou_val for c, iou_val in enumerate(metrics[\"Per-class IoU\"])}\n",
    "\n",
    "        # Ajout du num√©ro d'image\n",
    "        iou_dict[\"image_index\"] = i\n",
    "        iou_per_image.append(iou_dict)    \n",
    "\n",
    "    # Conversion en DataFrame pandas\n",
    "    df_iou = pd.DataFrame(iou_per_image)\n",
    "    df_iou.set_index(\"image_index\", inplace=True)\n",
    "    \n",
    "    print(f\"Mean IoU \\n{df_iou.mean().round(2)}\")\n",
    "    \n",
    "    df_iou_nonzero = df_iou.replace(0, np.nan)\n",
    "    # Calcul de la moyenne par classe en excluant les z√©ros\n",
    "    mean_iou_nonzero = df_iou_nonzero.mean(skipna=True)\n",
    "    print(f\"Mean IoU without zeros \\n{mean_iou_nonzero.round(2)}\")\n",
    "    print(f\"Mean IoU of all clases without zeros \\n{mean_iou_nonzero.mean():.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Aucun r√©sultat de segmentation √† afficher.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Workload:\n",
    "    n_images: int = 500_000  # nombre total d'images\n",
    "    days: int = 30  # p√©riode d'√©tude (jours)\n",
    "    hours_per_day: float = 24.0  # fen√™tre quotidienne d'activit√© (24 = fonctionnement continu)\n",
    "    ips: float | None = (\n",
    "        None  # images par seconde EFFECTIVES (si connu); si None, on ne calcule pas l'heure active minimale\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EndpointPlan:\n",
    "    name: str\n",
    "    hourly_rate_usd: float  # co√ªt $/h de l'instance (doc HF)\n",
    "    min_replicas: int = 1  # r√©plicas minimum toujours allum√©s (autoscaling min)\n",
    "    extra_replicas: int = 0  # r√©plicas additionnels pendant les pics (optionnel)\n",
    "    extra_hours_per_day: float = (\n",
    "        0.0  # nombre d'heures/jour o√π les r√©plicas additionnels sont actifs\n",
    "    )\n",
    "\n",
    "\n",
    "def total_hours(days: int, hours_per_day: float) -> float:\n",
    "    return days * hours_per_day\n",
    "\n",
    "\n",
    "def cost_flat(plan: EndpointPlan, days: int, hours_per_day: float) -> float:\n",
    "    \"\"\"\n",
    "    Co√ªt si on garde les r√©plicas min allum√©s toute la fen√™tre, avec √©ventuellement des r√©plicas\n",
    "    additionnels sur une fraction du temps (ex: pics).\n",
    "    \"\"\"\n",
    "    base = plan.hourly_rate_usd * plan.min_replicas * total_hours(days, hours_per_day)\n",
    "    extra = plan.hourly_rate_usd * plan.extra_replicas * (days * plan.extra_hours_per_day)\n",
    "    return base + extra\n",
    "\n",
    "\n",
    "def min_active_hours_needed(n_images: int, ips: float) -> float:\n",
    "    \"\"\"\n",
    "    Heures strictement n√©cessaires pour traiter n_images √† un d√©bit ips.\n",
    "    \"\"\"\n",
    "    if ips <= 0:\n",
    "        raise ValueError(\"ips doit √™tre > 0\")\n",
    "    seconds = n_images / ips\n",
    "    return seconds / 3600.0\n",
    "\n",
    "\n",
    "def cost_scale_to_zero_like(plan: EndpointPlan, needed_hours: float) -> float:\n",
    "    \"\"\"\n",
    "    Approximation d'un endpoint qui ne tourne que le temps 'utile' (scale-to-zero).\n",
    "    Dans la r√©alit√©, pr√©vois une marge (warmup, files d‚Äôattente).\n",
    "    \"\"\"\n",
    "    return plan.hourly_rate_usd * plan.min_replicas * needed_hours\n",
    "\n",
    "\n",
    "# ---------- Sc√©narios pr√™ts √† l'emploi ----------\n",
    "\n",
    "wl = Workload(n_images=500_000, days=30, hours_per_day=24.0)\n",
    "\n",
    "cpu_small = EndpointPlan(name=\"CPU aws intel-spr x2\", hourly_rate_usd=0.067, min_replicas=1)\n",
    "gpu_t4 = EndpointPlan(name=\"GPU aws nvidia-t4 x1\", hourly_rate_usd=0.5, min_replicas=1)\n",
    "\n",
    "# 1) Fonctionnement CONTINU (24/7) sur 30 jours (approche simple, sans supposer un d√©bit)\n",
    "cost_cpu_flat = cost_flat(cpu_small, wl.days, wl.hours_per_day)  # ~ 0.067 * 30 * 24\n",
    "cost_gpu_flat = cost_flat(gpu_t4, wl.days, wl.hours_per_day)  # ~ 0.5   * 30 * 24\n",
    "\n",
    "print(f\"[Flat 24/7] CPU intel-spr x2 (1 r√©plique): ${cost_cpu_flat:,.2f}\")\n",
    "print(f\"[Flat 24/7] GPU nvidia-t4 x1 (1 r√©plique): ${cost_gpu_flat:,.2f}\")\n",
    "\n",
    "# 2) Si tu connais un D√âBIT effectif (ips), on calcule le temps minimal 'utile'\n",
    "#    Exemples de d√©bits illustratifs (√† ajuster selon tes mesures r√©elles):\n",
    "for assumed_ips in [0.5, 1.0, 2.0, 5.0]:\n",
    "    needed_h = min_active_hours_needed(wl.n_images, assumed_ips)\n",
    "    cpu_cost_scaled = cost_scale_to_zero_like(cpu_small, needed_h)\n",
    "    gpu_cost_scaled = cost_scale_to_zero_like(gpu_t4, needed_h)\n",
    "    print(f\"\\n[Scale-to-zero approx] {assumed_ips} img/s\")\n",
    "    print(f\"Heures actives n√©cessaires ‚âà {needed_h:.1f} h\")\n",
    "    print(f\"Co√ªt CPU intel-spr x2 (1 r√©plique): ${cpu_cost_scaled:,.2f}\")\n",
    "    print(f\"Co√ªt GPU nvidia-t4 x1 (1 r√©plique): ${gpu_cost_scaled:,.2f}\")\n",
    "\n",
    "# 3) Exemple 'pics' : 1 r√©plique min 24/7 + 2 r√©plicas additionnels 2 h/jour (trafic concentr√©)\n",
    "cpu_bursty = EndpointPlan(\n",
    "    name=\"CPU bursty\",\n",
    "    hourly_rate_usd=0.067,\n",
    "    min_replicas=1,\n",
    "    extra_replicas=2,\n",
    "    extra_hours_per_day=2.0,\n",
    ")\n",
    "print(\n",
    "    f\"\\n[Bursty] CPU 1 r√©plique 24/7 + 2 r√©plicas pendant 2h/jour sur 30j : ${cost_flat(cpu_bursty, wl.days, wl.hours_per_day):,.2f}\"\n",
    ")\n",
    "\n",
    "# Photo instantan√©e : combien co√ªte un mois en continu pour chaque plan.\n",
    "labels = [\"CPU 24/7\", \"GPU 24/7\"]\n",
    "values = [cost_cpu_flat, cost_gpu_flat]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, values)\n",
    "plt.ylabel(\"Co√ªt total (USD) sur 30 jours\")\n",
    "plt.title(\"Comparaison des co√ªts 24/7\")\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v, f\"${v:,.0f}\", ha=\"center\", va=\"bottom\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_cell"
   },
   "source": [
    "## Conclusion et Prochaines √âtapes\n",
    "\n",
    "F√©licitations ! Vous avez appris √† :\n",
    "- Configurer les appels √† l'API d'inf√©rence Hugging Face.\n",
    "- Envoyer des images pour la segmentation.\n",
    "- Interpr√©ter les r√©sultats (avec l'aide des fonctions fournies).\n",
    "- Visualiser les segmentations.\n",
    "\n",
    "Pistes d'am√©lioration ou d'exploration :\n",
    "- **Gestion d'erreurs plus fine** : Impl√©menter des tentatives multiples (retry) en cas d'√©chec de l'API (par exemple, si le mod√®le est en cours de chargement).\n",
    "- **Appels asynchrones** : Pour un grand nombre d'images, des appels asynchrones (avec `asyncio` et `aiohttp`) seraient beaucoup plus rapides.\n",
    "- **Autres mod√®les** : Explorer d'autres mod√®les de segmentation ou d'autres t√¢ches sur Hugging Face Hub.\n",
    "\n",
    "N'h√©sitez pas √† modifier le code, √† tester avec vos propres images et √† explorer davantage !\n",
    "\n",
    "**_Note_** : Si vous aimez ce mod√®le, n'h√©sitez pas √† le [t√©l√©charger](https://huggingface.co/sayeed99/segformer_b3_clothes) et jouer avec directement sur votre machine !"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (FashionTrend)",
   "language": "python",
   "name": "fashiontrend_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
